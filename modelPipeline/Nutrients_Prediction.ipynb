{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUT3Y0OjupUJ",
        "outputId": "430fd7a5-a63a-451e-84d5-a7a3ca718cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Downloading Flask_Cors-5.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-5.0.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow keras numpy pandas flask flask-cors Pillow\n",
        "\n",
        "\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from flask import Flask, request, jsonify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"rkuo2000/uecfood100\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNgrStEtvQyq",
        "outputId": "2d104ad0-e5eb-4066-a0f0-6b36251727ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/rkuo2000/uecfood100?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 946M/946M [00:25<00:00, 38.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/rkuo2000/uecfood100/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = '/root/.cache/kagglehub/datasets/rkuo2000/uecfood100/versions/1'\n",
        "dest_dir = '/content/food-101'\n",
        "\n",
        "shutil.move(source_dir, dest_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Uo2EXYuVvX5I",
        "outputId": "eb083eaa-e090-473d-cb86-3cf8cceb2514"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/food-101'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Path to the folder where your downloaded dataset currently resides\n",
        "# (it should contain one folder per class)\n",
        "data_dir = '/content/food-101'\n",
        "\n",
        "# Create the destination train and test directories\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# List all class directories (skip the newly created train/test folders)\n",
        "class_names = [d for d in os.listdir(data_dir)\n",
        "               if os.path.isdir(os.path.join(data_dir, d)) and d not in ['train', 'test']]\n",
        "\n",
        "# For each class folder, split images 80% for train and 20% for test\n",
        "for class_name in class_names:\n",
        "    class_path = os.path.join(data_dir, class_name)\n",
        "\n",
        "    # Create subdirectories for this class in both train and test directories\n",
        "    train_class_dir = os.path.join(train_dir, class_name)\n",
        "    test_class_dir = os.path.join(test_dir, class_name)\n",
        "    os.makedirs(train_class_dir, exist_ok=True)\n",
        "    os.makedirs(test_class_dir, exist_ok=True)\n",
        "\n",
        "    # Get list of image files (adjust the filter if needed)\n",
        "    images = [f for f in os.listdir(class_path)\n",
        "              if os.path.isfile(os.path.join(class_path, f))]\n",
        "\n",
        "    # Shuffle and split images into train and test sets\n",
        "    random.shuffle(images)\n",
        "    split_idx = int(0.8 * len(images))\n",
        "    train_images = images[:split_idx]\n",
        "    test_images = images[split_idx:]\n",
        "\n",
        "    # Copy images to the corresponding train folder\n",
        "    for img in train_images:\n",
        "        src = os.path.join(class_path, img)\n",
        "        dst = os.path.join(train_class_dir, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "    # Copy images to the corresponding test folder\n",
        "    for img in test_images:\n",
        "        src = os.path.join(class_path, img)\n",
        "        dst = os.path.join(test_class_dir, img)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "print(\"Dataset successfully split into train and test directories.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X8B8o37vf_O",
        "outputId": "13f5ef9b-3c7e-4162-ae7a-fdfdd27789b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split into train and test directories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "# This is the folder that contains the 100 food category directories\n",
        "source_dir = '/content/food-101/UECFOOD100'\n",
        "# These folders were already created by your previous code\n",
        "train_dir = '/content/food-101/train'\n",
        "test_dir = '/content/food-101/test'\n",
        "\n",
        "# List only the subdirectories that represent the food classes.\n",
        "# Assuming class directories are named with digits (e.g., '1', '2', ..., '100')\n",
        "categories = [d for d in os.listdir(source_dir)\n",
        "              if os.path.isdir(os.path.join(source_dir, d)) and d.isdigit()]\n",
        "\n",
        "for category in categories:\n",
        "    cat_source = os.path.join(source_dir, category)\n",
        "\n",
        "    # Create corresponding subdirectories in train and test folders\n",
        "    cat_train = os.path.join(train_dir, category)\n",
        "    cat_test = os.path.join(test_dir, category)\n",
        "    os.makedirs(cat_train, exist_ok=True)\n",
        "    os.makedirs(cat_test, exist_ok=True)\n",
        "\n",
        "    # List only image files ending with '.jpg'\n",
        "    image_files = [f for f in os.listdir(cat_source)\n",
        "                   if os.path.isfile(os.path.join(cat_source, f)) and f.lower().endswith('.jpg')]\n",
        "\n",
        "    # Shuffle and split files: 80% for training, 20% for testing\n",
        "    random.shuffle(image_files)\n",
        "    split_idx = int(0.8 * len(image_files))\n",
        "    train_files = image_files[:split_idx]\n",
        "    test_files = image_files[split_idx:]\n",
        "\n",
        "    # Copy training images\n",
        "    for img in train_files:\n",
        "        src_path = os.path.join(cat_source, img)\n",
        "        dst_path = os.path.join(cat_train, img)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Copy testing images\n",
        "    for img in test_files:\n",
        "        src_path = os.path.join(cat_source, img)\n",
        "        dst_path = os.path.join(cat_test, img)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "print(\"Files copied into train and test directories for each category.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I89Qp6A2wipT",
        "outputId": "df09e641-1bec-43d3-fe50-2b0273ff1f54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files copied into train and test directories for each category.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Food-101 dataset\n",
        "train_dir = '/content/food-101/train'\n",
        "test_dir = '/content/food-101/test'\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    validation_split=0.2,\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='training')\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    subset='validation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbV2QI2UyjBb",
        "outputId": "c61814c0-162f-4a86-a32e-79820062696a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9199 images belonging to 101 classes.\n",
            "Found 2249 images belonging to 101 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create EfficientNet model\n",
        "base_model = tf.keras.applications.EfficientNetB3(\n",
        "    input_shape=(224, 224, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet')\n",
        "\n",
        "model = keras.Sequential([\n",
        "    base_model,\n",
        "    keras.layers.GlobalAveragePooling2D(),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(101, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=15,\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    validation_steps=len(val_generator))\n",
        "\n",
        "# Save model\n",
        "model.save('food_recognition.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHTw2Qq7zLSj",
        "outputId": "16889a63-353f-46b1-9bf5-bf2a10c836eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 857ms/step - accuracy: 0.0609 - loss: 4.4074 - val_accuracy: 0.0325 - val_loss: 4.5248\n",
            "Epoch 2/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 600ms/step - accuracy: 0.3369 - loss: 2.9169 - val_accuracy: 0.0751 - val_loss: 4.2310\n",
            "Epoch 3/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 595ms/step - accuracy: 0.5454 - loss: 1.8070 - val_accuracy: 0.1565 - val_loss: 4.0236\n",
            "Epoch 4/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 600ms/step - accuracy: 0.6563 - loss: 1.2805 - val_accuracy: 0.1423 - val_loss: 4.0818\n",
            "Epoch 5/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 605ms/step - accuracy: 0.7089 - loss: 1.0354 - val_accuracy: 0.2201 - val_loss: 3.7783\n",
            "Epoch 6/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 598ms/step - accuracy: 0.7489 - loss: 0.8715 - val_accuracy: 0.0258 - val_loss: 20.7016\n",
            "Epoch 7/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 596ms/step - accuracy: 0.7837 - loss: 0.7332 - val_accuracy: 0.4206 - val_loss: 2.6366\n",
            "Epoch 8/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 599ms/step - accuracy: 0.8086 - loss: 0.6469 - val_accuracy: 0.0938 - val_loss: 4.7221\n",
            "Epoch 9/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 591ms/step - accuracy: 0.8342 - loss: 0.5484 - val_accuracy: 0.3739 - val_loss: 3.0517\n",
            "Epoch 10/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 588ms/step - accuracy: 0.8493 - loss: 0.4908 - val_accuracy: 0.0351 - val_loss: 5.5679\n",
            "Epoch 11/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 587ms/step - accuracy: 0.8612 - loss: 0.4347 - val_accuracy: 0.4873 - val_loss: 2.5377\n",
            "Epoch 12/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 597ms/step - accuracy: 0.8692 - loss: 0.4001 - val_accuracy: 0.1494 - val_loss: 4.7075\n",
            "Epoch 13/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 588ms/step - accuracy: 0.8694 - loss: 0.3903 - val_accuracy: 0.0307 - val_loss: 629.0038\n",
            "Epoch 14/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 592ms/step - accuracy: 0.8871 - loss: 0.3372 - val_accuracy: 0.0213 - val_loss: 5.5537\n",
            "Epoch 15/15\n",
            "\u001b[1m288/288\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 587ms/step - accuracy: 0.8860 - loss: 0.3314 - val_accuracy: 0.4967 - val_loss: 2.5977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "6OyW448R8_Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"utsavdey1410/food-nutrition-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0rRuJ6kAE1P",
        "outputId": "0557731d-599f-45ac-a3c5-c9e80fba2ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/utsavdey1410/food-nutrition-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 694k/694k [00:00<00:00, 91.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/utsavdey1410/food-nutrition-dataset/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYOLgkQ-A56M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "source_dir = '/root/.cache/kagglehub/datasets/utsavdey1410/food-nutrition-dataset/versions/1'\n",
        "dest_dir = '/content/FINAL FOOD DATASET'\n",
        "\n",
        "shutil.move(source_dir, dest_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LZYWe19tqdGj",
        "outputId": "4f18c50b-dc8b-47c9-e6fb-094512e2085b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/FINAL FOOD DATASET'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(r\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP1.csv\")\n",
        "df2 = pd.read_csv(r\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP2.csv\")\n",
        "df3 = pd.read_csv(r\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP3.csv\")\n",
        "df4 = pd.read_csv(r\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP3.csv\")\n",
        "df5 = pd.read_csv(r\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP5.csv\")"
      ],
      "metadata": {
        "id": "ODehtmNABb30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2, df3, df4], ignore_index=True)"
      ],
      "metadata": {
        "id": "qVVadHtbB2-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ5xJ_U0B7UZ",
        "outputId": "6b6f3e2e-fdcd-4e35-d2ec-e0b877f56d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2012, 37)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of4fJQOtB71k",
        "outputId": "2c16a225-e4a8-441e-aa47-e07b387889d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0.1', 'Unnamed: 0', 'food', 'Caloric Value', 'Fat',\n",
              "       'Saturated Fats', 'Monounsaturated Fats', 'Polyunsaturated Fats',\n",
              "       'Carbohydrates', 'Sugars', 'Protein', 'Dietary Fiber', 'Cholesterol',\n",
              "       'Sodium', 'Water', 'Vitamin A', 'Vitamin B1', 'Vitamin B11',\n",
              "       'Vitamin B12', 'Vitamin B2', 'Vitamin B3', 'Vitamin B5', 'Vitamin B6',\n",
              "       'Vitamin C', 'Vitamin D', 'Vitamin E', 'Vitamin K', 'Calcium', 'Copper',\n",
              "       'Iron', 'Magnesium', 'Manganese', 'Phosphorus', 'Potassium', 'Selenium',\n",
              "       'Zinc', 'Nutrition Density'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Complete Nutrition Integration with Full Class Map\n",
        "\n",
        "# Create complete class mapping (ID to Food Name)\n",
        "class_map = [\n",
        "    {\"id\": 1, \"name\": \"rice\"},\n",
        "    {\"id\": 2, \"name\": \"eels on rice\"},\n",
        "    {\"id\": 3, \"name\": \"pilaf\"},\n",
        "    {\"id\": 4, \"name\": \"chicken-'n'-egg on rice\"},\n",
        "    {\"id\": 5, \"name\": \"pork cutlet on rice\"},\n",
        "    {\"id\": 6, \"name\": \"beef curry\"},\n",
        "    {\"id\": 7, \"name\": \"sushi\"},\n",
        "    {\"id\": 8, \"name\": \"chicken rice\"},\n",
        "    {\"id\": 9, \"name\": \"fried rice\"},\n",
        "    {\"id\": 10, \"name\": \"tempura bowl\"},\n",
        "    {\"id\": 11, \"name\": \"bibimbap\"},\n",
        "    {\"id\": 12, \"name\": \"toast\"},\n",
        "    {\"id\": 13, \"name\": \"croissant\"},\n",
        "    {\"id\": 14, \"name\": \"roll bread\"},\n",
        "    {\"id\": 15, \"name\": \"raisin bread\"},\n",
        "    {\"id\": 16, \"name\": \"chip butty\"},\n",
        "    {\"id\": 17, \"name\": \"hamburger\"},\n",
        "    {\"id\": 18, \"name\": \"pizza\"},\n",
        "    {\"id\": 19, \"name\": \"sandwiches\"},\n",
        "    {\"id\": 20, \"name\": \"udon noodle\"},\n",
        "    {\"id\": 21, \"name\": \"tempura udon\"},\n",
        "    {\"id\": 22, \"name\": \"soba noodle\"},\n",
        "    {\"id\": 23, \"name\": \"ramen noodle\"},\n",
        "    {\"id\": 24, \"name\": \"beef noodle\"},\n",
        "    {\"id\": 25, \"name\": \"tensin noodle\"},\n",
        "    {\"id\": 26, \"name\": \"fried noodle\"},\n",
        "    {\"id\": 27, \"name\": \"spaghetti\"},\n",
        "    {\"id\": 28, \"name\": \"Japanese-style pancake\"},\n",
        "    {\"id\": 29, \"name\": \"takoyaki\"},\n",
        "    {\"id\": 30, \"name\": \"gratin\"},\n",
        "    {\"id\": 31, \"name\": \"sauteed vegetables\"},\n",
        "    {\"id\": 32, \"name\": \"croquette\"},\n",
        "    {\"id\": 33, \"name\": \"grilled eggplant\"},\n",
        "    {\"id\": 34, \"name\": \"sauteed spinach\"},\n",
        "    {\"id\": 35, \"name\": \"vegetable tempura\"},\n",
        "    {\"id\": 36, \"name\": \"miso soup\"},\n",
        "    {\"id\": 37, \"name\": \"potage\"},\n",
        "    {\"id\": 38, \"name\": \"sausage\"},\n",
        "    {\"id\": 39, \"name\": \"oden\"},\n",
        "    {\"id\": 40, \"name\": \"omelet\"},\n",
        "    {\"id\": 41, \"name\": \"ganmodoki\"},\n",
        "    {\"id\": 42, \"name\": \"jiaozi\"},\n",
        "    {\"id\": 43, \"name\": \"stew\"},\n",
        "    {\"id\": 44, \"name\": \"teriyaki grilled fish\"},\n",
        "    {\"id\": 45, \"name\": \"fried fish\"},\n",
        "    {\"id\": 46, \"name\": \"grilled salmon\"},\n",
        "    {\"id\": 47, \"name\": \"salmon meuniere\"},\n",
        "    {\"id\": 48, \"name\": \"sashimi\"},\n",
        "    {\"id\": 49, \"name\": \"grilled pacific saury\"},\n",
        "    {\"id\": 50, \"name\": \"sukiyaki\"},\n",
        "    {\"id\": 51, \"name\": \"sweet and sour pork\"},\n",
        "    {\"id\": 52, \"name\": \"lightly roasted fish\"},\n",
        "    {\"id\": 53, \"name\": \"steamed egg hotchpotch\"},\n",
        "    {\"id\": 54, \"name\": \"tempura\"},\n",
        "    {\"id\": 55, \"name\": \"fried chicken\"},\n",
        "    {\"id\": 56, \"name\": \"sirloin cutlet\"},\n",
        "    {\"id\": 57, \"name\": \"nanbanzuke\"},\n",
        "    {\"id\": 58, \"name\": \"boiled fish\"},\n",
        "    {\"id\": 59, \"name\": \"seasoned beef with potatoes\"},\n",
        "    {\"id\": 60, \"name\": \"hambarg steak\"},\n",
        "    {\"id\": 61, \"name\": \"beef steak\"},\n",
        "    {\"id\": 62, \"name\": \"dried fish\"},\n",
        "    {\"id\": 63, \"name\": \"ginger pork saute\"},\n",
        "    {\"id\": 64, \"name\": \"spicy chili-flavored tofu\"},\n",
        "    {\"id\": 65, \"name\": \"yakitori\"},\n",
        "    {\"id\": 66, \"name\": \"cabbage roll\"},\n",
        "    {\"id\": 67, \"name\": \"rolled omelet\"},\n",
        "    {\"id\": 68, \"name\": \"egg sunny-side up\"},\n",
        "    {\"id\": 69, \"name\": \"fermented soybeans\"},\n",
        "    {\"id\": 70, \"name\": \"cold tofu\"},\n",
        "    {\"id\": 71, \"name\": \"egg roll\"},\n",
        "    {\"id\": 72, \"name\": \"chilled noodle\"},\n",
        "    {\"id\": 73, \"name\": \"stir-fried beef and peppers\"},\n",
        "    {\"id\": 74, \"name\": \"simmered pork\"},\n",
        "    {\"id\": 75, \"name\": \"boiled chicken and vegetables\"},\n",
        "    {\"id\": 76, \"name\": \"sashimi bowl\"},\n",
        "    {\"id\": 77, \"name\": \"sushi bowl\"},\n",
        "    {\"id\": 78, \"name\": \"fish-shaped pancake with bean jam\"},\n",
        "    {\"id\": 79, \"name\": \"shrimp with chill source\"},\n",
        "    {\"id\": 80, \"name\": \"roast chicken\"},\n",
        "    {\"id\": 81, \"name\": \"steamed meat dumpling\"},\n",
        "    {\"id\": 82, \"name\": \"omelet with fried rice\"},\n",
        "    {\"id\": 83, \"name\": \"cutlet curry\"},\n",
        "    {\"id\": 84, \"name\": \"spaghetti meat sauce\"},\n",
        "    {\"id\": 85, \"name\": \"fried shrimp\"},\n",
        "    {\"id\": 86, \"name\": \"potato salad\"},\n",
        "    {\"id\": 87, \"name\": \"green salad\"},\n",
        "    {\"id\": 88, \"name\": \"macaroni salad\"},\n",
        "    {\"id\": 89, \"name\": \"Japanese tofu and vegetable chowder\"},\n",
        "    {\"id\": 90, \"name\": \"pork miso soup\"},\n",
        "    {\"id\": 91, \"name\": \"chinese soup\"},\n",
        "    {\"id\": 92, \"name\": \"beef bowl\"},\n",
        "    {\"id\": 93, \"name\": \"kinpira-style sauteed burdock\"},\n",
        "    {\"id\": 94, \"name\": \"rice ball\"},\n",
        "    {\"id\": 95, \"name\": \"pizza toast\"},\n",
        "    {\"id\": 96, \"name\": \"dipping noodles\"},\n",
        "    {\"id\": 97, \"name\": \"hot dog\"},\n",
        "    {\"id\": 98, \"name\": \"french fries\"},\n",
        "    {\"id\": 99, \"name\": \"mixed rice\"},\n",
        "    {\"id\": 100, \"name\": \"goya chanpuru\"}\n",
        "]\n",
        "\n",
        "class_df = pd.DataFrame(class_map)\n",
        "\n",
        "# Load and clean nutrition data\n",
        "df1 = pd.read_csv(\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP1.csv\")\n",
        "df2 = pd.read_csv(\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP2.csv\")\n",
        "df3 = pd.read_csv(\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP3.csv\")\n",
        "df4 = pd.read_csv(\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP4.csv\")  # Fixed\n",
        "df5 = pd.read_csv(\"/content/FINAL FOOD DATASET/FINAL FOOD DATASET/FOOD-DATA-GROUP5.csv\")\n",
        "\n",
        "nutrition_df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
        "\n",
        "# Clean nutrition data\n",
        "nutrition_df = nutrition_df.rename(columns={'food': 'Food'})\n",
        "nutrition_df['Food'] = nutrition_df['Food'].str.lower().str.strip()\n",
        "nutrition_df = nutrition_df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Nutrition Density'], errors='ignore')\n",
        "\n",
        "# Create improved nutrition mapping with fuzzy matching\n",
        "!pip install fuzzywuzzy python-Levenshtein\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "nutrition_mapping = {}\n",
        "food_choices = nutrition_df['Food'].unique().tolist()\n",
        "\n",
        "for _, row in class_df.iterrows():\n",
        "    target_name = row['name'].lower()\n",
        "\n",
        "    # Fuzzy match with score threshold\n",
        "    match, score = process.extractOne(target_name, food_choices, scorer=process.fuzz.token_sort_ratio)\n",
        "\n",
        "    if score > 75:  # Adjust threshold as needed\n",
        "        nutrition_data = nutrition_df[nutrition_df['Food'] == match].iloc[0].to_dict()\n",
        "        nutrition_mapping[row['id']] = nutrition_data\n",
        "    else:\n",
        "        print(f\"No match found for {row['name']} (best match: {match}@{score})\")\n",
        "        nutrition_mapping[row['id']] = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK9WrgHqFs0X",
        "outputId": "9189cb1c-bc2b-43a1-ac89-a8d029626e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.26.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.26.1 rapidfuzz-3.12.1\n",
            "No match found for rice (best match: corn rice@62)\n",
            "No match found for eels on rice (best match: corn rice@67)\n",
            "No match found for pilaf (best match: sapodilla@57)\n",
            "No match found for chicken-'n'-egg on rice (best match: chicken egg roll@70)\n",
            "No match found for pork cutlet on rice (best match: pork loin raw@62)\n",
            "No match found for beef curry (best match: beef jerky@70)\n",
            "No match found for sushi (best match: succotash@57)\n",
            "No match found for tempura bowl (best match: black tea@57)\n",
            "No match found for bibimbap (best match: biscuit mcdonalds@40)\n",
            "No match found for toast (best match: oats@67)\n",
            "No match found for croissant (best match: apple croissant@75)\n",
            "No match found for roll bread (best match: rye bread@74)\n",
            "No match found for chip butty (best match: cashew butter@61)\n",
            "No match found for pizza (best match: pizza sauce@62)\n",
            "No match found for sandwiches (best match: steak sandwich@75)\n",
            "No match found for udon noodle (best match: chocolate donut@62)\n",
            "No match found for tempura udon (best match: sturgeon raw@50)\n",
            "No match found for soba noodle (best match: beef noodle soup@67)\n",
            "No match found for ramen noodle (best match: ramen noodle soup dry@73)\n",
            "No match found for tensin noodle (best match: ramen noodle soup dry@59)\n",
            "No match found for fried noodle (best match: oyster fried@67)\n",
            "No match found for Japanese-style pancake (best match: japanese persimmon@60)\n",
            "No match found for takoyaki (best match: potato skin raw@52)\n",
            "No match found for gratin (best match: gin@67)\n",
            "No match found for croquette (best match: croutons@59)\n",
            "No match found for grilled eggplant (best match: eggplant raw@71)\n",
            "No match found for vegetable tempura (best match: vegetable soup@71)\n",
            "No match found for miso soup (best match: minestrone soup@75)\n",
            "No match found for potage (best match: orange@67)\n",
            "No match found for sausage (best match: sausage pizza@70)\n",
            "No match found for oden (best match: fondant@55)\n",
            "No match found for ganmodoki (best match: almond oil@63)\n",
            "No match found for jiaozi (best match: jeijoa@50)\n",
            "No match found for stew (best match: oyster raw@57)\n",
            "No match found for teriyaki grilled fish (best match: chuck eye steak grilled@64)\n",
            "No match found for grilled salmon (best match: salmon cooked@67)\n",
            "No match found for salmon meuniere (best match: salmon raw@64)\n",
            "No match found for sashimi (best match: tahini@62)\n",
            "No match found for grilled pacific saury (best match: ham patties grilled@60)\n",
            "No match found for sukiyaki (best match: teriyaki sauce@64)\n",
            "No match found for lightly roasted fish (best match: cashew nuts roasted@62)\n",
            "No match found for steamed egg hotchpotch (best match: poached egg@61)\n",
            "No match found for tempura (best match: tempeh@62)\n",
            "No match found for sirloin cutlet (best match: lamb sirloin cooked@61)\n",
            "No match found for nanbanzuke (best match: abalone@59)\n",
            "No match found for boiled fish (best match: fish broth@67)\n",
            "No match found for seasoned beef with potatoes (best match: sandwich with roast beef@71)\n",
            "No match found for ginger pork saute (best match: ginger root@64)\n",
            "No match found for spicy chili-flavored tofu (best match: chicken flavored rice raw@60)\n",
            "No match found for yakitori (best match: daiquiri@50)\n",
            "No match found for rolled omelet (best match: omelet@63)\n",
            "No match found for egg sunny-side up (best match: egg substitute powder@53)\n",
            "No match found for fermented soybeans (best match: soybean curd cheese@65)\n",
            "No match found for cold tofu (best match: trout cooked@67)\n",
            "No match found for stir-fried beef and peppers (best match: sweet green peppers canned@64)\n",
            "No match found for simmered pork (best match: pork skin@64)\n",
            "No match found for boiled chicken and vegetables (best match: mixed vegetables canned@69)\n",
            "No match found for sashimi bowl (best match: shrimp raw@55)\n",
            "No match found for sushi bowl (best match: brown sugar@57)\n",
            "No match found for fish-shaped pancake with bean jam (best match: pancakes with butter syrup@61)\n",
            "No match found for shrimp with chill source (best match: hotdog with chili@59)\n",
            "No match found for steamed meat dumpling (best match: sesame meal@56)\n",
            "No match found for omelet with fried rice (best match: fried rice@62)\n",
            "No match found for cutlet curry (best match: curry powder@58)\n",
            "No match found for green salad (best match: cornsalad@70)\n",
            "No match found for Japanese tofu and vegetable chowder (best match: vegetable chicken soup@60)\n",
            "No match found for pork miso soup (best match: shark fin soup@64)\n",
            "No match found for kinpira-style sauteed burdock (best match: pork country style ribs raw@54)\n",
            "No match found for pizza toast (best match: sausage pizza@67)\n",
            "No match found for dipping noodles (best match: chinese noodles@67)\n",
            "No match found for hot dog (best match: hotdog roll@56)\n",
            "No match found for goya chanpuru (best match: tomato puree canned@56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " MANUAL_NUTRITION_DB = {\n",
        " 1: {'name': 'rice', 'calories': 130, 'protein': 2.7, 'carbs': 28, 'fat': 0.3},\n",
        "    2: {'name': 'eels on rice', 'calories': 450, 'protein': 20, 'carbs': 60, 'fat': 12},\n",
        "    3: {'name': 'pilaf', 'calories': 350, 'protein': 8, 'carbs': 45, 'fat': 15},\n",
        "    4: {'name': 'chicken-\\'n\\'-egg on rice', 'calories': 600, 'protein': 25, 'carbs': 75, 'fat': 20},\n",
        "    5: {'name': 'pork cutlet on rice', 'calories': 700, 'protein': 30, 'carbs': 80, 'fat': 25},\n",
        "    6: {'name': 'beef curry', 'calories': 650, 'protein': 22, 'carbs': 70, 'fat': 28},\n",
        "    7: {'name': 'sushi', 'calories': 200, 'protein': 12, 'carbs': 40, 'fat': 4},  # 4 pieces\n",
        "    8: {'name': 'chicken rice', 'calories': 400, 'protein': 20, 'carbs': 45, 'fat': 15},\n",
        "    9: {'name': 'fried rice', 'calories': 500, 'protein': 15, 'carbs': 60, 'fat': 20},\n",
        "    10: {'name': 'tempura bowl', 'calories': 750, 'protein': 18, 'carbs': 85, 'fat': 35},\n",
        "    11: {'name': 'bibimbap', 'calories': 550, 'protein': 20, 'carbs': 65, 'fat': 20},\n",
        "\n",
        "    # Breads (12-16)\n",
        "    12: {'name': 'toast', 'calories': 75, 'protein': 3, 'carbs': 12, 'fat': 1},\n",
        "    13: {'name': 'croissant', 'calories': 230, 'protein': 5, 'carbs': 25, 'fat': 12},\n",
        "    14: {'name': 'roll bread', 'calories': 150, 'protein': 4, 'carbs': 28, 'fat': 2},\n",
        "    15: {'name': 'raisin bread', 'calories': 180, 'protein': 4, 'carbs': 33, 'fat': 3},\n",
        "    16: {'name': 'chip butty', 'calories': 400, 'protein': 7, 'carbs': 45, 'fat': 22},\n",
        "\n",
        "    # Western Dishes (17-19)\n",
        "    17: {'name': 'hamburger', 'calories': 295, 'protein': 17, 'carbs': 24, 'fat': 14},\n",
        "    18: {'name': 'pizza', 'calories': 1140, 'protein': 48, 'carbs': 132, 'fat': 40},  # whole pizza\n",
        "    19: {'name': 'sandwiches', 'calories': 300, 'protein': 15, 'carbs': 30, 'fat': 12},\n",
        "\n",
        "    # Noodles (20-27)\n",
        "    20: {'name': 'udon noodle', 'calories': 350, 'protein': 10, 'carbs': 60, 'fat': 5},\n",
        "    21: {'name': 'tempura udon', 'calories': 600, 'protein': 15, 'carbs': 75, 'fat': 25},\n",
        "    22: {'name': 'soba noodle', 'calories': 300, 'protein': 12, 'carbs': 55, 'fat': 4},\n",
        "    23: {'name': 'ramen noodle', 'calories': 400, 'protein': 15, 'carbs': 65, 'fat': 12},\n",
        "    24: {'name': 'beef noodle', 'calories': 450, 'protein': 20, 'carbs': 50, 'fat': 18},\n",
        "    25: {'name': 'tensin noodle', 'calories': 380, 'protein': 18, 'carbs': 55, 'fat': 10},\n",
        "    26: {'name': 'fried noodle', 'calories': 450, 'protein': 12, 'carbs': 60, 'fat': 18},\n",
        "    27: {'name': 'spaghetti', 'calories': 220, 'protein': 8, 'carbs': 43, 'fat': 1},\n",
        "\n",
        "    # Japanese Specialties (28-50)\n",
        "    28: {'name': 'Japanese-style pancake', 'calories': 350, 'protein': 15, 'carbs': 30, 'fat': 20},\n",
        "    29: {'name': 'takoyaki', 'calories': 320, 'protein': 20, 'carbs': 40, 'fat': 12},  # 4 pieces\n",
        "    30: {'name': 'gratin', 'calories': 400, 'protein': 18, 'carbs': 35, 'fat': 22},\n",
        "    31: {'name': 'sauteed vegetables', 'calories': 120, 'protein': 3, 'carbs': 15, 'fat': 7},\n",
        "    32: {'name': 'croquette', 'calories': 150, 'protein': 5, 'carbs': 15, 'fat': 8},  # per piece\n",
        "    33: {'name': 'grilled eggplant', 'calories': 50, 'protein': 1, 'carbs': 12, 'fat': 0.2},\n",
        "    34: {'name': 'sauteed spinach', 'calories': 40, 'protein': 3, 'carbs': 6, 'fat': 1},\n",
        "    35: {'name': 'vegetable tempura', 'calories': 320, 'protein': 12, 'carbs': 32, 'fat': 16},  # 4 pieces\n",
        "    36: {'name': 'miso soup', 'calories': 35, 'protein': 2, 'carbs': 5, 'fat': 1},\n",
        "    37: {'name': 'potage', 'calories': 150, 'protein': 4, 'carbs': 20, 'fat': 6},\n",
        "    38: {'name': 'sausage', 'calories': 150, 'protein': 7, 'carbs': 2, 'fat': 13},\n",
        "    39: {'name': 'oden', 'calories': 150, 'protein': 8, 'carbs': 12, 'fat': 7},\n",
        "    40: {'name': 'omelet', 'calories': 154, 'protein': 11, 'carbs': 2, 'fat': 12},\n",
        "     # Continued from previous\n",
        "    41: {'name': 'ganmodoki', 'calories': 180, 'protein': 8, 'carbs': 15, 'fat': 12},\n",
        "    42: {'name': 'jiaozi', 'calories': 50, 'protein': 3, 'carbs': 6, 'fat': 2},  # per dumpling\n",
        "    43: {'name': 'stew', 'calories': 250, 'protein': 15, 'carbs': 20, 'fat': 12},\n",
        "    44: {'name': 'teriyaki grilled fish', 'calories': 280, 'protein': 25, 'carbs': 15, 'fat': 14},\n",
        "    45: {'name': 'fried fish', 'calories': 350, 'protein': 20, 'carbs': 25, 'fat': 20},\n",
        "    46: {'name': 'grilled salmon', 'calories': 300, 'protein': 27, 'carbs': 0, 'fat': 18},\n",
        "    47: {'name': 'salmon meuniere', 'calories': 320, 'protein': 28, 'carbs': 10, 'fat': 20},\n",
        "    48: {'name': 'sashimi', 'calories': 120, 'protein': 20, 'carbs': 0, 'fat': 5},  # per 100g\n",
        "    49: {'name': 'grilled pacific saury', 'calories': 250, 'protein': 22, 'carbs': 0, 'fat': 17},\n",
        "    50: {'name': 'sukiyaki', 'calories': 600, 'protein': 35, 'carbs': 25, 'fat': 35},\n",
        "\n",
        "    51: {'name': 'sweet and sour pork', 'calories': 380, 'protein': 18, 'carbs': 35, 'fat': 20},\n",
        "    52: {'name': 'lightly roasted fish', 'calories': 200, 'protein': 25, 'carbs': 0, 'fat': 10},\n",
        "    53: {'name': 'steamed egg hotchpotch', 'calories': 150, 'protein': 12, 'carbs': 8, 'fat': 9},\n",
        "    54: {'name': 'tempura', 'calories': 80, 'protein': 3, 'carbs': 8, 'fat': 4},  # per piece\n",
        "    55: {'name': 'fried chicken', 'calories': 320, 'protein': 20, 'carbs': 15, 'fat': 22},\n",
        "    56: {'name': 'sirloin cutlet', 'calories': 450, 'protein': 30, 'carbs': 20, 'fat': 28},\n",
        "    57: {'name': 'nanbanzuke', 'calories': 280, 'protein': 18, 'carbs': 15, 'fat': 16},\n",
        "    58: {'name': 'boiled fish', 'calories': 180, 'protein': 25, 'carbs': 0, 'fat': 8},\n",
        "    59: {'name': 'seasoned beef with potatoes', 'calories': 400, 'protein': 22, 'carbs': 35, 'fat': 20},\n",
        "    60: {'name': 'hambarg steak', 'calories': 350, 'protein': 25, 'carbs': 15, 'fat': 24},\n",
        "\n",
        "    61: {'name': 'beef steak', 'calories': 400, 'protein': 35, 'carbs': 0, 'fat': 28},\n",
        "    62: {'name': 'dried fish', 'calories': 150, 'protein': 30, 'carbs': 0, 'fat': 3},\n",
        "    63: {'name': 'ginger pork saute', 'calories': 380, 'protein': 28, 'carbs': 15, 'fat': 25},\n",
        "    64: {'name': 'spicy chili-flavored tofu', 'calories': 180, 'protein': 12, 'carbs': 10, 'fat': 10},\n",
        "    65: {'name': 'yakitori', 'calories': 120, 'protein': 10, 'carbs': 3, 'fat': 8},  # per skewer\n",
        "    66: {'name': 'cabbage roll', 'calories': 200, 'protein': 12, 'carbs': 15, 'fat': 10},\n",
        "    67: {'name': 'rolled omelet', 'calories': 150, 'protein': 10, 'carbs': 5, 'fat': 10},\n",
        "    68: {'name': 'egg sunny-side up', 'calories': 90, 'protein': 6, 'carbs': 0, 'fat': 7},\n",
        "    69: {'name': 'fermented soybeans', 'calories': 200, 'protein': 15, 'carbs': 20, 'fat': 8},\n",
        "    70: {'name': 'cold tofu', 'calories': 80, 'protein': 8, 'carbs': 2, 'fat': 5},\n",
        "\n",
        "    71: {'name': 'egg roll', 'calories': 100, 'protein': 7, 'carbs': 5, 'fat': 6},  # per piece\n",
        "    72: {'name': 'chilled noodle', 'calories': 300, 'protein': 10, 'carbs': 50, 'fat': 8},\n",
        "    73: {'name': 'stir-fried beef and peppers', 'calories': 280, 'protein': 22, 'carbs': 15, 'fat': 16},\n",
        "    74: {'name': 'simmered pork', 'calories': 350, 'protein': 25, 'carbs': 10, 'fat': 25},\n",
        "    75: {'name': 'boiled chicken and vegetables', 'calories': 250, 'protein': 20, 'carbs': 20, 'fat': 10},\n",
        "    76: {'name': 'sashimi bowl', 'calories': 500, 'protein': 35, 'carbs': 60, 'fat': 15},\n",
        "    77: {'name': 'sushi bowl', 'calories': 550, 'protein': 25, 'carbs': 75, 'fat': 18},\n",
        "    78: {'name': 'fish-shaped pancake with bean jam', 'calories': 120, 'protein': 3, 'carbs': 25, 'fat': 2},\n",
        "    79: {'name': 'shrimp with chill source', 'calories': 200, 'protein': 18, 'carbs': 10, 'fat': 10},\n",
        "    80: {'name': 'roast chicken', 'calories': 300, 'protein': 35, 'carbs': 0, 'fat': 18},\n",
        "\n",
        "    81: {'name': 'steamed meat dumpling', 'calories': 80, 'protein': 5, 'carbs': 10, 'fat': 3},  # per dumpling\n",
        "    82: {'name': 'omelet with fried rice', 'calories': 600, 'protein': 25, 'carbs': 70, 'fat': 25},\n",
        "    83: {'name': 'cutlet curry', 'calories': 700, 'protein': 30, 'carbs': 80, 'fat': 30},\n",
        "    84: {'name': 'spaghetti meat sauce', 'calories': 400, 'protein': 18, 'carbs': 50, 'fat': 15},\n",
        "    85: {'name': 'fried shrimp', 'calories': 100, 'protein': 8, 'carbs': 5, 'fat': 6},  # per shrimp\n",
        "    86: {'name': 'potato salad', 'calories': 180, 'protein': 4, 'carbs': 20, 'fat': 10},\n",
        "    87: {'name': 'green salad', 'calories': 50, 'protein': 2, 'carbs': 8, 'fat': 1},\n",
        "    88: {'name': 'macaroni salad', 'calories': 200, 'protein': 5, 'carbs': 25, 'fat': 10},\n",
        "    89: {'name': 'japanese tofu and vegetable chowder', 'calories': 150, 'protein': 8, 'carbs': 15, 'fat': 7},\n",
        "    90: {'name': 'pork miso soup', 'calories': 120, 'protein': 10, 'carbs': 8, 'fat': 6},\n",
        "\n",
        "    91: {'name': 'chinese soup', 'calories': 100, 'protein': 6, 'carbs': 10, 'fat': 4},\n",
        "    92: {'name': 'beef bowl', 'calories': 650, 'protein': 30, 'carbs': 80, 'fat': 25},\n",
        "    93: {'name': 'kinpira-style sauteed burdock', 'calories': 120, 'protein': 3, 'carbs': 20, 'fat': 5},\n",
        "    94: {'name': 'rice ball', 'calories': 150, 'protein': 3, 'carbs': 33, 'fat': 0.5},\n",
        "    95: {'name': 'pizza toast', 'calories': 300, 'protein': 12, 'carbs': 35, 'fat': 15},\n",
        "    96: {'name': 'dipping noodles', 'calories': 350, 'protein': 12, 'carbs': 60, 'fat': 8},\n",
        "    97: {'name': 'hot dog', 'calories': 250, 'protein': 10, 'carbs': 18, 'fat': 15},\n",
        "    98: {'name': 'french fries', 'calories': 312, 'protein': 4, 'carbs': 41, 'fat': 15},\n",
        "    99: {'name': 'mixed rice', 'calories': 400, 'protein': 8, 'carbs': 75, 'fat': 10},\n",
        "    100: {'name': 'goya chanpuru', 'calories': 180, 'protein': 8, 'carbs': 15, 'fat': 12}\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "k3rvTX7TLBDs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nutrition(image_path):\n",
        "    # Load and preprocess image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(IMG_SIZE)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) / 255.0\n",
        "\n",
        "    # Get model prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class_id = predicted_class_idx + 1  # Convert to 1-100 ID system\n",
        "\n",
        "    # Get nutrition data\n",
        "    nutrition_data = MANUAL_NUTRITION_DB.get(predicted_class_id,\n",
        "                                          MANUAL_NUTRITION_DB[100])  # Fallback to last item\n",
        "\n",
        "    # Adjust for typical serving sizes\n",
        "    serving_adjustments = {\n",
        "        'sushi': 4,         # 4 pieces\n",
        "        'takoyaki': 4,      # 4 pieces\n",
        "        'tempura': 4,       # 4 pieces\n",
        "        'croquette': 2,     # 2 pieces\n",
        "        'pizza': 0.25       # 1 slice\n",
        "    }\n",
        "\n",
        "    food_name = nutrition_data['name']\n",
        "    if food_name in serving_adjustments:\n",
        "        multiplier = serving_adjustments[food_name]\n",
        "        adjusted_data = {k: v*multiplier if k != 'name' else v\n",
        "                        for k, v in nutrition_data.items()}\n",
        "        nutrition_data = adjusted_data\n",
        "\n",
        "    return {\n",
        "        'food_id': predicted_class_id,\n",
        "        'food_class': nutrition_data['name'],\n",
        "        'confidence': float(np.max(predictions)),\n",
        "        'nutrition': {\n",
        "            'calories': nutrition_data['calories'],\n",
        "            'protein': nutrition_data['protein'],\n",
        "            'carbohydrates': nutrition_data['carbs'],\n",
        "            'fat': nutrition_data['fat']\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "9xHtykfAzR4x"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with sample image\n",
        "test_image_path = \"/content/Screenshot 2025-02-15 173208.png\"\n",
        "result = predict_nutrition(test_image_path)\n",
        "\n",
        "print(\"Predicted Class:\", result['food_class'])\n",
        "print(\"Confidence:\", result['confidence'])\n",
        "print(\"Nutrition Info:\", result['nutrition'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvF8l7JFPUEL",
        "outputId": "3e5f28c0-8f55-4c2c-dbd2-b84c4632d88f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "Predicted Class: toast\n",
            "Confidence: 0.9997493624687195\n",
            "Nutrition Info: {'calories': 75, 'protein': 3, 'carbohydrates': 12, 'fat': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-XOORRCTQcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads'\n",
        "app.config['ALLOWED_EXTENSIONS'] = {'png', 'jpg', 'jpeg'}\n",
        "\n",
        "# Load model\n",
        "model = tf.keras.models.load_model('food_recognition.h5')\n",
        "\n",
        "def allowed_file(filename):\n",
        "    return '.' in filename and \\\n",
        "           filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def handle_prediction():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "\n",
        "    if file.filename == '':\n",
        "        return jsonify({'error': 'Empty filename'}), 400\n",
        "\n",
        "    if not allowed_file(file.filename):\n",
        "        return jsonify({'error': 'Invalid file type'}), 400\n",
        "\n",
        "    try:\n",
        "        filename = secure_filename(file.filename)\n",
        "        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "        file.save(filepath)\n",
        "\n",
        "        result = predict_nutrition(filepath)\n",
        "        os.remove(filepath)\n",
        "\n",
        "        return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)\n",
        "    app.run(host='0.0.0.0', port=5000, threaded=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFISTi6L892b",
        "outputId": "89316013-7c2b-4cd2-f34e-e76983ffeae1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JZk0QdG6gT4M",
        "outputId": "7705137d-cfea-4838-9963-983e64d8184b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_nutrition(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(IMG_SIZE)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0) / 255.0\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    top_class = class_names[np.argmax(predictions[0])]\n",
        "\n",
        "    if top_class.lower() in nutrition_mapping:\n",
        "        nutrition = nutrition_mapping[top_class.lower()].to_dict()\n",
        "    else:\n",
        "        nutrition = None\n",
        "\n",
        "    return {\n",
        "        'food_class': top_class,\n",
        "        'confidence': float(np.max(predictions)),\n",
        "        'nutrition': nutrition\n",
        "    }"
      ],
      "metadata": {
        "id": "TWd5Bgfczcx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P87HV1jTPOTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from werkzeug.utils import secure_filename\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['UPLOAD_FOLDER'] = 'uploads/'\n",
        "model = tf.keras.models.load_model('/content/food_recognition.h5')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def api_predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error': 'No file uploaded'}), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    filename = secure_filename(file.filename)\n",
        "    filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n",
        "    file.save(filepath)\n",
        "\n",
        "    try:\n",
        "        result = predict_nutrition(filepath)\n",
        "        return jsonify(result)\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "RV5jwK5Q0BLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rK5-Rgc0Fwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}